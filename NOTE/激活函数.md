#ComputerScience/AI/ML/DL   
下面是一些常见的激活函数.  

# 1. Sigmoid  
$$Sigmoid(x) = \frac{1}{1 + e^{-x}}$$
可以将输入映射到 $0 \sim 1$ 之间.
适合作为二分类问题的最后一层激活函数.  

# 2. Tanh  
$$Tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$
可以将输入映射到 $-1 \sim 1$ 之间.  

# 3. ReLU  
> [!important] 此函数是深度学习中最常用的激活函数.


$$ReLU(x) = \max(x , 0)$$
- 如果输入大于 0, 则输出 x.
- 如果输入小于等于 0, 则输出 0.



# 4. Leaky ReLU
ReLU 函数的问题在于当输入小于 0 时输出为 0, 没有办法继续计算梯度.  
于是人们提出 Leaky ReLU 函数, 其定义为:  
$$
f(n) =  
\begin{cases}  
x & \text{if} \ x > 0 \\  
\alpha x & \text{if} \ x \leq 0 \\  
\end{cases}
$$
其中, $\alpha$ 一般是不大于 1 的正数.  

# 5. Softmax  
$$o_i = \frac{e^{z_i}}{\sum^{n}_{k = 1} e^{z_k}} $$
输出范围是 $0 \sim 1$ .  
这个函数的好处是保证输出值全是正数, 以及可以放大不同 Logits 之间的差异.  
一般我们认为此函数的输出映射到概率. 


  
 
