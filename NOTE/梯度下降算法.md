 #ComputerScience/AI/ML   
# 1. 算法逻辑  

1. 初始化  
对于每一个要优化的参数, 给定初始值.  
2. 计算梯度  
根据当前参数值, 让 $Loss$ 函数对每个参数求偏导.  
3. 迭代更新  
让当前参数向量沿着梯度负方向, 用**学习率乘以梯度的值**前进, 然后再计算梯度.重复此过程.  
4. 停止条件  
可以设置迭代次数, 达到次数后结束; 也可以设置若 $Loss$ 连续 $n$ 次不减小认为达到最小值, 同样结束.  


> [!note] 损失函数的概念主要在监督学习中出现.  




每个参数在每次梯度下降迭代时,都和损失函数的偏导数的值相关.损失函数不能和样本个数相关.  

为了让训练稳定，为了让不同的样本数也有差不多大小的偏导数值,一般在 $Loss$ 函数中会除 $Loss$ 样本数.

对于回归问题，一般采用的是均方误差 (Mean Squared Error,MSE).  
其公式为: $$MSE=\frac{1}{n}\sum_{i=1}^n​(y^i​-\hat{y^i}​)^2$$  
其中 $y^i$ 是第 $i$ 个样本点的特征, $\hat{y^i}$ 是模型对第 $i$ 个样本点的预测值.  



